# Numerical-Optimization-for-Machine-Learning

- implement the gradient Descent variants:
    - Batch
    - mini-Batch
    - Stochastic

- Implement the Accelerated Gradient Descent methods with adaptive learning rate:
    - Adagrad
    - RMSProp
    - Adam

- Implement the Accelerated Gradient Descent methods:
    - Momentum
    - NAG

Applying these optimization techniques to linear regression for single and multi-variable problems.

