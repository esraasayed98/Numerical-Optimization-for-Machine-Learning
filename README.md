# Numerical-Optimization-for-Machine-Learning

- implement the gradient Descent variants:
    - Batch
    - mini-BAtch
    - Stocastic

- Implement the Accelerated Gradient Descent methods with adaptive learning rate:
    - Adagrad
    - RMSProp
    - Adam

- Implement the accelerated Gradient Descent methods:
    - Momentum
    - NAG

Applying these optimization techniques to linear regression for single and multi-variable problems.

